# -*- coding: utf-8 -*-
"""Q/A chatbot with LLMs + Harry Potter

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/q-a-chatbot-with-llms-harry-potter-2be970e1-e657-4151-ac82-c211c0993bf7.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240220/auto/storage/goog4_request%26X-Goog-Date%3D20240220T151040Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D16b6c21720b3dbfc919828792679744057903e42fc90f966af22e16f922356429e6994fe75a588d82a9ac584347b4806563293a3e5aadce04ea7df90832c574d18fadd28fbdb409f66a851f65c2251fe48d3cdd1fb907b8055e7920efa3ff1147a413bf86253032b46816ce5b4c4f3640c9f0fb3acc247400e4342cd4583e2d6f991af54d483b8d96f390b8370523ec886fb250a867f1dcbf5b9d121dd4cd23f1cec5b62a2cdd96f5825dfb66ada7c9bb3812b26cb5b1fc9e0132fbd512f7739baec181a8e75da12c3bb7e079192772e7222c69e961c72f3a251132eabe4b3fc6b1c6c3ff2c7d487626160f9bf7e7543780e6f0bae135b483cdbeec0936b1ae0
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'harry-potter-books-in-pdf-1-7:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3327967%2F5793551%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240220%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240220T151040Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5d4b77aa7f1fdb4a7779d9675af1be7cbac16d470ba59475c604b11947bbb00b1fc7d52d0ffd7fb9eb1cb7ce645c5f510da8d807da552fb9980e09220bcf1ba05117f595e97d9e495ad4e17766005ca6ef114f6ca99c6d747f89fe5bccd0541f4960f15c5df144fc904fc7e58270158a6c4cce1184ecd578c8ea60bac637eedea23d28d475639c0f7502061ba88e12dafe6c2169696b494b12617ac123922247ab1250e89a1f29f5d645cc20624df7999cfcba66cc9ff3f211e22822b418499c1820b099cab7e9cb831612c09180564d13375647655006fc8cc5a4362cbc684460a99d13c351db17af8d71e216062bafc64d3de5aa9e0ccd9725327488acbe1e,faiss-hp-sentence-transformers:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3669776%2F6369513%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240220%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240220T151040Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8f260b725bf9626cd6062a36a0d92b74ea6b830aef066fd47194f5e48f7cb00b4716f1a144188a33e444797154bebe5bb61c4fb0919f30f8028b7ce0ba158c4ed8d73aaa8c93c464596e2af27ca1784356417090701822396dea5b6f8cf0482a2d8a934d2c26ce5dfc5767ffaf4643ab3950c1ee7fc439271d83ba8aa725d86d8fdc519a71a6a0f544fbf9b1dd4d06aea6632d114f397d4d9b17b3b1900e5de36ebb08978b4f9bf5e708aa9f240686a10a2a0dd53719419ae2809ec8183eb4cca26bfece396d20db3bab28f44ec72bb66c6e6d5e25d58e8de6bc300eac7f2b0667375371e587499f7f45aac74118f72aa4a946a3d213f7b105e4b09dc05c3704'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""# Overview

- Use [Langchain](https://python.langchain.com/en/latest/index.html) to **<font color='orange'>build a chatbot that can answer questions about</font>** [Harry Potter books](https://www.kaggle.com/datasets/hinepo/harry-potter-books-in-pdf-1-7)
- **<font color='orange'>Flexible and customizable RAG pipeline (Retrieval Augmented Generation)</font>**
- Experiment with various LLMs (Large Language Models)
- Use [FAISS vector store](https://python.langchain.com/docs/integrations/vectorstores/faiss) to store text embeddings created with [Sentence Transformers](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) from ðŸ¤—. FAISS runs on GPU and it is much faster than Chroma
- Use [Retrieval chain](https://python.langchain.com/docs/modules/data_connection/retrievers/) to retrieve relevant passages from embedded text
- Summarize retrieved passages
- Leverage Kaggle dual GPU (2 * T4) with [Hugging Face Accelerate](https://huggingface.co/docs/accelerate/index)
- Chat UI with [Gradio](https://www.gradio.app/guides/quickstart)

**<font color='green'>No need to create any API key to use this notebook! Everything is open source.</font>**

**<font color='orange'>Don't forget to upvote the notebook if you learn from it or use it!</font>**

### Models

- [TheBloke/wizardLM-7B-HF](https://huggingface.co/TheBloke/wizardLM-7B-HF)
- [daryl149/llama-2-7b-chat-hf](https://huggingface.co/daryl149/llama-2-7b-chat-hf)
- [daryl149/llama-2-13b-chat-hf](https://huggingface.co/daryl149/llama-2-13b-chat-hf)
- [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)

![image.png](attachment:cdc462a7-e241-4332-821a-fa369a853128.png)

img source: HinePo
"""

! nvidia-smi -L

"""# Installs"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from IPython.display import clear_output
# 
# ! pip install sentence_transformers==2.2.2
# 
# ! pip install -qq -U langchain
# ! pip install -qq -U tiktoken
# ! pip install -qq -U pypdf
# ! pip install -qq -U faiss-gpu
# ! pip install -qq -U InstructorEmbedding
# 
# ! pip install -qq -U transformers
# ! pip install -qq -U accelerate
# ! pip install -qq -U bitsandbytes
# 
# clear_output()

"""# Imports"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# import warnings
# warnings.filterwarnings("ignore")
# 
# import os
# import glob
# import textwrap
# import time
# 
# import langchain
# 
# # loaders
# from langchain.document_loaders import PyPDFLoader
# from langchain.document_loaders import DirectoryLoader
# 
# # splits
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# 
# # prompts
# from langchain import PromptTemplate, LLMChain
# 
# # vector stores
# from langchain.vectorstores import FAISS
# 
# # models
# from langchain.llms import HuggingFacePipeline
# from langchain.embeddings import HuggingFaceInstructEmbeddings
# 
# # retrievers
# from langchain.chains import RetrievalQA
# 
# import torch
# import transformers
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# 
# print('langchain:', langchain.__version__)
# print('torch:', torch.__version__)
# print('transformers:', transformers.__version__)

sorted(glob.glob('/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/*'))

"""# CFG

- CFG class enables easy and organized experimentation
"""

class CFG:
    # LLMs
    model_name = 'llama2-13b-chat' # wizardlm, llama2-7b-chat, llama2-13b-chat, mistral-7B
    temperature = 0,
    top_p = 0.95,
    repetition_penalty = 1.15

    # splitting
    split_chunk_size = 800
    split_overlap = 0

    # embeddings
    embeddings_model_repo = 'sentence-transformers/all-MiniLM-L6-v2'

    # similar passages
    k = 3

    # paths
    PDFs_path = '/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/'
    Embeddings_path =  '/kaggle/input/faiss-hp-sentence-transformers'
    Persist_directory = './harry-potter-vectordb'

"""# Define model"""

def get_model(model = CFG.model_name):

    print('\nDownloading model: ', model, '\n\n')

    if model == 'wizardlm':
        model_repo = 'TheBloke/wizardLM-7B-HF'

        tokenizer = AutoTokenizer.from_pretrained(model_repo)

        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            load_in_4bit = True,
            device_map = 'auto',
            torch_dtype = torch.float16,
            low_cpu_mem_usage = True
        )

        max_len = 1024

    elif model == 'llama2-7b-chat':
        model_repo = 'daryl149/llama-2-7b-chat-hf'

        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)

        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            load_in_4bit = True,
            device_map = 'auto',
            torch_dtype = torch.float16,
            low_cpu_mem_usage = True,
            trust_remote_code = True
        )

        max_len = 2048

    elif model == 'llama2-13b-chat':
        model_repo = 'daryl149/llama-2-13b-chat-hf'

        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)

        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            load_in_4bit = True,
            device_map = 'auto',
            torch_dtype = torch.float16,
            low_cpu_mem_usage = True,
            trust_remote_code = True
        )

        max_len = 2048 # 8192

    elif model == 'mistral-7B':
        model_repo = 'mistralai/Mistral-7B-v0.1'

        tokenizer = AutoTokenizer.from_pretrained(model_repo)

        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            load_in_4bit = True,
            device_map = 'auto',
            torch_dtype = torch.float16,
            low_cpu_mem_usage = True,
        )

        max_len = 1024

    else:
        print("Not implemented model (tokenizer and backbone)")

    return tokenizer, model, max_len

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# tokenizer, model, max_len = get_model(model = CFG.model_name)

"""# ðŸ¤— pipeline

- Hugging Face pipeline
"""

pipe = pipeline(
    task = "text-generation",
    model = model,
    tokenizer = tokenizer,
    pad_token_id = tokenizer.eos_token_id,
    max_length = max_len,
    temperature = CFG.temperature,
    top_p = CFG.top_p,
    repetition_penalty = CFG.repetition_penalty
)

llm = HuggingFacePipeline(pipeline = pipe)

"""# ðŸ¦œðŸ”— Langchain

- Multiple document retriever with LangChain
"""

CFG.model_name

"""# Loader

- [Directory loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory) for multiple files
- This step is not necessary if you are just loading the vector database
- This step is necessary if you are creating embeddings. In this case you need to:
    - load de PDF files
    - split into chunks
    - create embeddings
    - save the embeddings in a vector store
    - After that you can just load the saved embeddings to do similarity search with the user query, and then use the LLM to answer the question
    
You can comment out this section if you use the embeddings I already created.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# loader = DirectoryLoader(
#     CFG.PDFs_path,
#     glob="./*.pdf",
#     loader_cls=PyPDFLoader,
#     show_progress=True,
#     use_multithreading=True
# )
# 
# documents = loader.load()

"""# Splitter

- Splitting the text into chunks so its passages are easily searchable for similarity
- This step is also only necessary if you are creating the embeddings
- [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/reference/modules/document_loaders.html?highlight=RecursiveCharacterTextSplitter#langchain.document_loaders.MWDumpLoader)
"""

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = CFG.split_chunk_size,
    chunk_overlap = CFG.split_overlap
)

texts = text_splitter.split_documents(documents)

print(f'We have created {len(texts)} chunks from {len(documents)} pages')

"""# Create Embeddings


- Embedd and store the texts in a Vector database (FAISS)
- [LangChain Vector Stores docs](https://python.langchain.com/docs/modules/data_connection/vectorstores/)
- [FAISS - langchain](https://python.langchain.com/docs/integrations/vectorstores/faiss)
- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks - paper Aug/2019](https://arxiv.org/pdf/1908.10084.pdf)
- [This is a nice 4 minutes video about vector stores](https://www.youtube.com/watch?v=dN0lsF2cvm4)
- [Chroma - Persist and load the vector database](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html)

___

- If you use Chroma vector store it will take ~35 min to create embeddings
- If you use FAISS vector store on GPU it will take just ~3 min

___

We need to create the embeddings only once, and then we can just load the vector store and query the database using similarity search.

Loading the embeddings takes only a few seconds.

I uploaded the embeddings to a Kaggle Dataset so we just load it from [here](https://www.kaggle.com/datasets/hinepo/faiss-hp-sentence-transformers).
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# ### download embeddings model
# embeddings = HuggingFaceInstructEmbeddings(
#     model_name = CFG.embeddings_model_repo,
#     model_kwargs = {"device": "cuda"}
# )
# 
# ### create embeddings and DB
# vectordb = FAISS.from_documents(
#     documents = texts,
#     embedding = embeddings
# )
# 
# ### persist vector database
# vectordb.save_local("faiss_index_hp")

"""# Load vector database

- After saving the vector database, we just load it from the Kaggle Dataset I mentioned
- Obviously, the embeddings function to load the embeddings must be the same as the one used to create the embeddings
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# ### download embeddings model
# embeddings = HuggingFaceInstructEmbeddings(
#     model_name = CFG.embeddings_model_repo,
#     model_kwargs = {"device": "cuda"}
# )
# 
# ### load vector DB embeddings
# vectordb = FAISS.load_local(
#     CFG.Embeddings_path,
#     embeddings
# )

"""# Prompt Template

- Custom prompt
"""

prompt_template = """
Don't try to make up an answer, if you don't know just say that you don't know.
Answer in the same language the question was asked.
Use only the following pieces of context to answer the question at the end.

{context}

Question: {question}
Answer:"""


PROMPT = PromptTemplate(
    template = prompt_template,
    input_variables = ["context", "question"]
)

llm_chain = LLMChain(prompt=PROMPT, llm=llm)

"""# Retriever chain

- Retriever to retrieve relevant passages
- Chain to answer questions
- [RetrievalQA: Chain for question-answering](https://python.langchain.com/docs/modules/data_connection/retrievers/)
"""

retriever = vectordb.as_retriever(search_kwargs = {"k": CFG.k, "search_type" : "similarity"})

qa_chain = RetrievalQA.from_chain_type(
    llm = llm,
    chain_type = "stuff", # map_reduce, map_rerank, stuff, refine
    retriever = retriever,
    chain_type_kwargs = {"prompt": PROMPT},
    return_source_documents = True,
    verbose = False
)

"""# Post-process outputs

- Format llm response
- Cite sources (PDFs)
- Change `width` parameter to format the output
"""

def wrap_text_preserve_newlines(text, width=700):
    # Split the input text into lines based on newline characters
    lines = text.split('\n')

    # Wrap each line individually
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

    # Join the wrapped lines back together using newline characters
    wrapped_text = '\n'.join(wrapped_lines)

    return wrapped_text


def process_llm_response(llm_response):
    ans = wrap_text_preserve_newlines(llm_response['result'])

    sources_used = ' \n'.join(
        [
            source.metadata['source'].split('/')[-1][:-4] + ' - page: ' + str(source.metadata['page'])
            for source in llm_response['source_documents']
        ]
    )

    ans = ans + '\n\nSources: \n' + sources_used
    return ans

def llm_ans(query):
    start = time.time()

    llm_response = qa_chain.invoke(query)
    ans = process_llm_response(llm_response)

    end = time.time()

    time_elapsed = int(round(end - start, 0))
    time_elapsed_str = f'\n\nTime elapsed: {time_elapsed} s'
    return ans + time_elapsed_str

"""# Ask questions

- Question Answering from multiple documents
- Invoke QA Chain
- Talk to your data
"""

CFG.model_name

query = "Which challenges does Harry face during the Triwizard Tournament?"
print(llm_ans(query))

query = "Who is Hagrid?"
print(llm_ans(query))

query = "Is Malfoy an ally of Voldemort?"
print(llm_ans(query))

query = "What are horcrux?"
print(llm_ans(query))

query = "Give me 5 examples of cool potions and explain what they do"
print(llm_ans(query))

! pip install streamlit

! pip show streamlit

import streamlit

"""# Gradio Chat UI

- **<font color='orange'>At the moment this part only works on Google Colab. Gradio and Kaggle started having compatibility issues recently.</font>**
- If you plan to use the interface, it is preferable to do so in Google Colab
- I'll leave this section commented out for now
- Chat UI prints below

___

- Create a chat UI with [Gradio](https://www.gradio.app/guides/quickstart)
- [ChatInterface docs](https://www.gradio.app/docs/chatinterface)
- The notebook should be running if you want to use the chat interface
"""

import locale
locale.getpreferredencoding = lambda: "UTF-8"

! pip install --upgrade gradio -qq
clear_output()

! pip install typing_extensions --upgrade

import gradio as gr

def predict(message, history):
    # output = message # debug mode

    output = str(llm_ans(message)).replace("\n", "<br/>")
    return output

demo = gr.ChatInterface(
    predict,
    title = f' Open-Source LLM ({CFG.model_name}) for Harry Potter Question Answering'
)

demo.queue()
demo.launch()

"""![image.png](attachment:413fe7a3-6534-45b5-b6e3-7fc86e982cf1.png)

![image.png](attachment:976f4bf4-7626-4d4a-b773-3eebd7e9f000.png)

# Conclusions

- Feel free to fork and optimize the code. Lots of things can be improved.

- Things I found had the most impact on models output quality in my experiments:
    - Prompt engineering
    - Bigger models
    - Other models families
    - Splitting: chunk size, overlap
    - Search: Similarity, MMR, k
    - Pipeline parameters (temperature, top_p, penalty)
    - Embeddings function
    - LLM parameters (max len)


- LangChain, Hugging Face and Gradio are awesome libs!

- **<font color='orange'>Upvote if you liked it or want me to keep updating it with new models and functionalities</font>**

- If you are interested in **<font color='blue'>Instruction Finetuning for LLMs</font>**, you might also want to check my other [notebook](https://www.kaggle.com/code/hinepo/llm-instruction-finetuning-wandb)

ðŸ¦œðŸ”—ðŸ¤—

![image.png](attachment:68773819-4358-4ded-be3e-f1d275103171.png)
"""
